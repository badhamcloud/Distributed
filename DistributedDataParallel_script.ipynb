{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import DataParallel, Linear\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test, train_ds, test_ds = None,None,None,None\n",
    "def loaddata(rootdir = './MNIST_data'):\n",
    "    global mnist_train, mnist_test, train_ds, test_ds\n",
    "    print('loading data')\n",
    "    mnist_train = torchvision.datasets.MNIST(root = rootdir,train=True,download=True)\n",
    "    mnist_test = torchvision.datasets.MNIST(root = rootdir,train=False,download=True)\n",
    "    mnist_train,mnist_test\n",
    "\n",
    "    #A dataset returning pil image and label for each __getitem__ call\n",
    "    print(mnist_train)\n",
    "    train_ds = MyDataset(mnist_train)\n",
    "    test_ds = MyDataset(mnist_test)\n",
    "    print(len(train_ds), len(test_ds))\n",
    "    print(train_ds[0][0].shape,train_ds[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Images to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        super(MyDataset,self).__init__()\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "    def __getitem__(self, index):\n",
    "        item = self.mnist_dataset[index]\n",
    "        image = item[0]\n",
    "        label = item[1]\n",
    "        image_array = np.array(image,dtype = np.float32).reshape((-1))/255\n",
    "        return image_array, label\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, verbose = False):\n",
    "        super(LinearModel,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size,128)\n",
    "        self.fc2 = torch.nn.Linear(128,64)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc3 = torch.nn.Linear(64,output_size)\n",
    "        self.verbose = verbose\n",
    "    def forward(self, X):\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('X shape = {}, current cuda device = {}' \\\n",
    "                  .format(X.shape,torch.cuda.current_device() if torch.cuda.is_available() else 'NAN'))\n",
    "        h1 = self.relu(self.fc1(X))\n",
    "        return self.fc3(self.relu(self.fc2(h1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    print('using device',device)\n",
    "    losses = []\n",
    "    for epoch in range(args.epochs):\n",
    "        for x,y in tqdm(dataloader, position=0):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            yhat = model(x)\n",
    "            \n",
    "            loss = torch.nn.CrossEntropyLoss()(yhat, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        print ('loss at epoch {} is {}'.format(epoch, losses[-1]))\n",
    "        for param in model.parameters():\n",
    "            print('shape = {}, mean of values = {}'.format(param.shape,param.mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    losses,preds,labels = [],np.empty((0,)),np.empty((0,))\n",
    "    for x,y in tqdm(dataloader, position=0):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        yhat = model(x)\n",
    "        loss = torch.nn.CrossEntropyLoss()(yhat, y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        preds = np.hstack(  ( preds, np.argmax(yhat.detach().to('cpu').numpy(), axis = 1) .squeeze()) )\n",
    "        labels = np.hstack( (labels, y.to('cpu').numpy()) )\n",
    "        \n",
    "        #The losses are averaged across observations for each minibatch.\n",
    "    final_loss = sum(losses)/len(losses)\n",
    "    acc = accuracy(preds,labels)\n",
    "    print ('validation loss is {}'.format(final_loss))\n",
    "    print ('validation accuracy is {}'.format(acc))\n",
    "    \n",
    "    return final_loss, acc\n",
    "    \n",
    "\n",
    "def accuracy (preds, labels):\n",
    "    print(preds[:10], labels[:10])\n",
    "    return np.mean(preds == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_singlegpu():\n",
    "    gpu = torch.device('cuda')\n",
    "    model = LinearModel(784,10)\n",
    "    model = model.to(gpu)\n",
    "    dataloader = DataLoader(train_ds,batch_size=args.batch)\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=args.lr)\n",
    "    train(model, optimizer, dataloader, gpu)\n",
    "    \n",
    "    eval_dataloader = dataloader = DataLoader(test_ds, batch_size=args.batch)\n",
    "    evaluate(model,eval_dataloader, gpu)\n",
    "    \n",
    "    \n",
    "# import argparse\n",
    "# args = argparse.Namespace(\n",
    "#     batch = 16,\n",
    "#     lr = 0.001,\n",
    "#     epochs =1\n",
    "# )\n",
    "#loaddata()\n",
    "#train_singlegpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed data parallel\n",
    "Each GPU can be run in it's own process.\n",
    "\n",
    "1. Models are not transferred. \n",
    "1. Each process has it's own model initialized. \n",
    "1. Only gradients are summed up using all_reduce operation\n",
    "1. The seed should be same everywhere for consistency\n",
    "    \n",
    "This should be faster than Dataparallel. The only comppinication cost is all reduce.\n",
    "\n",
    "Tutorial: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n",
    "\n",
    "Data sampler source : https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler\n",
    "\n",
    "\n",
    "Backend https://pytorch.org/docs/stable/distributed.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile dist_training.py\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import os,torch,sys\n",
    "import datetime\n",
    "\n",
    "def setup(local_rank, global_rank, world_size, master_node_addr = 'localhost', master_node_port = '12355',backend = 'gloo'):\n",
    "    \n",
    "    print('setting up')\n",
    "    os.environ['MASTER_ADDR'] =master_node_addr\n",
    "    os.environ['MASTER_PORT'] = master_node_port\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(backend, rank=global_rank, world_size=world_size, timeout=datetime.timedelta(0,seconds =  300))\n",
    "    #dist.init_process_group(backend)\n",
    "    # Explicitly setting seed to make sure that models created in two processes\n",
    "    # start from same random weights and biases.\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "def train_distrib(local_rank, global_rank, world_size, master_addr = 'localhost', master_port = '12355',backend = 'gloo'):\n",
    "    print('inside train_distrib rank {} and world_size {}'.format(global_rank,world_size))\n",
    "    loaddata('./MNIST_data/{}/'.format(local_rank))\n",
    "    #initialize process group and set seed\n",
    "    setup(local_rank,global_rank,world_size, master_addr , master_port,backend )\n",
    "    \n",
    "    device_id = local_rank\n",
    "    \n",
    "    #Initialize model in it's own device. Can be any device. \n",
    "    #Better to initialize in own device for avoiding memory constraints (theory)\n",
    "    model = LinearModel(784,10,verbose = False).to(device_id)\n",
    "    \n",
    "    #Use Distributed data parallel. Check the documentation of DistributedDataParallel?\n",
    "    ddp_model = DistributedDataParallel(model,device_ids=[device_id], output_device=device_id)\n",
    "    \n",
    "    #Check where all_reduce happens\n",
    "    optimizer = torch.optim.Adam(ddp_model.parameters(), lr = args.lr)\n",
    "    \n",
    "    #A sampler is necessary as each process should work only on a subset of data. \n",
    "    #No need to pass rank if process group is initialized. Uses rank = dist.get_rank() to get rank\n",
    "    distributed_sampler = DistributedSampler(train_ds, num_replicas = world_size)\n",
    "    \n",
    "    dataloader = DataLoader(train_ds,batch_size=args.batch, sampler= distributed_sampler)\n",
    "    train(ddp_model, optimizer, dataloader, device_id)\n",
    "    \n",
    "    #only evaluate on first node\n",
    "    if global_rank == 0:\n",
    "        eval_dataloader = dataloader = DataLoader(test_ds, batch_size=args.batch)\n",
    "        eval_loss, acc = evaluate(model,eval_dataloader, device_id)\n",
    "        if args.isaml:\n",
    "            from azureml.core.run import Run\n",
    "            run = Run.get_context()\n",
    "            run.log('eval_loss', eval_loss)\n",
    "            run.log('acc', acc)\n",
    "    \n",
    "    cleanup()\n",
    "    \n",
    "#train_distrib(0,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--local_rank', type=int, help = 'Rank of the process')\n",
    "    parser.add_argument('--global_rank', type=int, help = 'Rank of the process')\n",
    "    parser.add_argument('--size',type=int, help = 'world size')\n",
    "    parser.add_argument('--issingle',action='store_true')\n",
    "    \n",
    "    parser.add_argument('--batch',type=int, help = 'batch size', default = 16)\n",
    "    parser.add_argument('--lr',type=float, help = 'learning rate', default = 0.001)\n",
    "    parser.add_argument('--epochs',type=int, help = 'total epochs', default = 1)\n",
    "    parser.add_argument('--isaml',action='store_true')\n",
    "    parser.add_argument('--backend',type=str, default = 'gloo')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    print('args = ',args)\n",
    "    if args.isaml:\n",
    "        print('environment variables = ', os.environ)\n",
    "    \n",
    "    #runs for both AML and local\n",
    "    if args.issingle:\n",
    "        print('Single GPU training')\n",
    "        loaddata()\n",
    "        train_singlegpu()\n",
    "        \n",
    "    \n",
    "    #AML and distributed multi node\n",
    "    elif args.isaml:\n",
    "        local_rank = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "        global_rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "        size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "        \n",
    "        #NCCL environment. Still works without it.\n",
    "        os.environ['NCCL_SOCKET_IFNAME'] = '^docker0,lo'\n",
    "        \n",
    "        master_node_params = os.environ['AZ_BATCH_MASTER_NODE'].split(':')        \n",
    "        master_node_addr = master_node_params[0]\n",
    "        master_node_port = master_node_params[1]\n",
    "        print(local_rank,global_rank,size,master_node_addr,master_node_port)\n",
    "        train_distrib(local_rank,global_rank, size,master_node_addr, master_node_port, args.backend)\n",
    "    \n",
    "    #distributed without AML. Single node only\n",
    "    else:\n",
    "        print('starting distibuted training', args)\n",
    "        train_distrib(args.local_rank, args.global_rank, args.size,backend = args.backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed attempts at Creating subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Create 4 processes and start training.\n",
    "\n",
    "\n",
    "\n",
    "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
    "\n",
    "Sharing CUDA tensors\n",
    "Sharing CUDA tensors between processes is supported only in Python 3, using a spawn or forkserver start methods. multiprocessing in Python 2 can only create subprocesses using fork, and it’s not supported by the CUDA runtime.\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/multiprocessing.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch.multiprocessing import Process\n",
    "def test (rank, size):\n",
    "    print('Inside. rank = {}, size = {}'.format(rank,size))\n",
    "def startprocesses(function, ranks, size):\n",
    "    processes = []\n",
    "    for rank in ranks:\n",
    "        p = Process(target=function, args=(rank, size))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    print(processes)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    print('Processes finished execution')\n",
    "    \n",
    "#startprocesses(test, [0,1],2)\n",
    "\n",
    "#One process\n",
    "#%time startprocesses(train_distrib, [0],1)\n",
    "\n",
    "#Two processes\n",
    "#%time startprocesses(train_distrib, [0,1],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No logs received in mp spawn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def run_demo(demo_fn, world_size):\n",
    "    p = mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "    print(p.processes)\n",
    "#run_demo(train_distrib, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
