{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import DataParallel, Linear\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np\n",
    "import  matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(root = './MNIST_data',train=True,download=True)\n",
    "mnist_train\n",
    "\n",
    "#A dataset returning pil image and label for each __getitem__ call\n",
    "print(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,20))\n",
    "count = 7\n",
    "for i in range(count):\n",
    "    plt.subplot(1,count,i+1)\n",
    "    index = np.random.randint(0,len(mnist_train),(1,))[0]\n",
    "    im_array = np.asarray(mnist_train[index][0])\n",
    "    label = mnist_train[index][1]\n",
    "    imshow(im_array,cmap='gray',)\n",
    "    plt.title(str(index) + ' label:'+str(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Images to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        super(MyDataset,self).__init__()\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "    def __getitem__(self, index):\n",
    "        item = self.mnist_dataset[index]\n",
    "        image = item[0]\n",
    "        label = item[1]\n",
    "        image_array = np.array(image,dtype = np.float32).reshape((-1))/255\n",
    "        return image_array, label\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MyDataset(mnist_train)\n",
    "print(train_ds[0][0].shape,train_ds[0][1])\n",
    "train_ds[0][0][200:250],train_ds[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace(\n",
    "    batch = 16,\n",
    "    lr = 0.001,\n",
    "    epochs =1\n",
    ")\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, verbose = False):\n",
    "        super(LinearModel,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size,64)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(64,output_size)\n",
    "        self.verbose = verbose\n",
    "    def forward(self, X):\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('X shape = {}, current cuda device = {}' \\\n",
    "                  .format(X.shape,torch.cuda.current_device() if torch.cuda.is_available() else 'NAN'))\n",
    "        return self.fc2(self.relu(self.fc1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = torch.Size([784]), current cuda device = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0154,  0.1979,  0.1550,  0.0505,  0.0616, -0.0447, -0.0850, -0.0380,\n",
       "        -0.1357, -0.1196], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LinearModel(784,10,True)\n",
    "m(torch.Tensor(train_ds[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_ds, batch_size=args.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7])]\n"
     ]
    }
   ],
   "source": [
    "#converts arrays into tensor and creates batches\n",
    "for data in dataloader:\n",
    "    print(data)\n",
    "    #print(model(data[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    print('using device',device)\n",
    "    losses = []\n",
    "    for epoch in range(args.epochs):\n",
    "        for x,y in tqdm(dataloader, position=0):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            yhat = model(x)\n",
    "            \n",
    "            loss = torch.nn.CrossEntropyLoss()(yhat, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        print ('loss at epoch {} is {}'.format(epoch, losses[-1]))\n",
    "        for param in model.parameters():\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(784,10)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3750 [00:00<09:08,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:20<00:00, 187.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is 0.018243134021759033\n",
      "CPU times: user 23.5 s, sys: 17.5 s, total: 41.1 s\n",
      "Wall time: 20 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cpu = torch.device('cpu')\n",
    "train(model,optimizer,dataloader, cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 38/3750 [00:00<00:09, 374.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:10<00:00, 352.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is 0.031419962644577026\n",
      "CPU times: user 10.7 s, sys: 28.2 ms, total: 10.7 s\n",
      "Wall time: 10.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.device('cuda')\n",
    "model = LinearModel(784,10)\n",
    "model = model.to(gpu)\n",
    "dataloader = DataLoader(train_ds,batch_size=args.batch)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=args.lr)\n",
    "\n",
    "%time train(model, optimizer, dataloader, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 14/938 [00:00<00:06, 134.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:06<00:00, 140.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is 0.056974977254867554\n",
      "CPU times: user 6.73 s, sys: 0 ns, total: 6.73 s\n",
      "Wall time: 6.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataloader = DataLoader(train_ds,batch_size=args.batch *4)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=args.lr*4)\n",
    "%time train(model, optimizer, dataloader, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataParallel\n",
    "single process, multiple GPUs\n",
    "\n",
    "    This container parallelizes the application of the given :attr:`module` by\n",
    "    splitting the input across the specified devices by chunking in the batch\n",
    "    dimension (other objects will be copied once per device). In the forward\n",
    "    pass, the module is replicated on each device, and each replica handles a\n",
    "    portion of the input. During the backwards pass, gradients from each replica\n",
    "    are summed into the original module.\n",
    "\n",
    "Source: https://pytorch.org/docs/stable/_modules/torch/nn/parallel/data_parallel.html\n",
    "\n",
    "The batch size should be larger than the number of GPUs used.\n",
    "\n",
    "\n",
    "1. This is not very optimal as every forward run transfers the model and data portion between GPUs. \n",
    "1. Unless the processing in the module takes significant amount of time, this is not very useful.\n",
    "1. This is even slower than single GPU for simpler neural nets.\n",
    "\n",
    "        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
    "        if len(self.device_ids) == 1:\n",
    "            return self.module(*inputs[0], **kwargs[0])\n",
    "        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n",
    "        outputs = self.parallel_apply(replicas, inputs, kwargs)\n",
    "        return self.gather(outputs, self.output_device)\n",
    "\n",
    "Issues : https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/3750 [00:00<00:22, 169.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:21<00:00, 176.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is 0.02713838219642639\n",
      "CPU times: user 25.4 s, sys: 2.39 s, total: 27.8 s\n",
      "Wall time: 21.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel(784,10,verbose = False).to(gpu)\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "#Each batch is split into number of gpus and data is scattered. Use verbose = True to verify parallel processing\n",
    "dataloader = DataLoader(train_ds,batch_size=args.batch)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=args.lr)\n",
    "\n",
    "%time train(model, optimizer, dataloader, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Higher batch size\n",
    "Even for higher batch sizes, it's still slower than single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 12/938 [00:00<00:08, 110.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:08<00:00, 110.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is 0.02199617028236389\n",
      "CPU times: user 9.62 s, sys: 620 ms, total: 10.2 s\n",
      "Wall time: 8.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Batch size can be increased to take advantage of the parallel processing\n",
    "dataloader = DataLoader(train_ds,batch_size=args.batch *4)\n",
    "#increase the lr for high batch size. (Provide proof for this)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=args.lr * 4)\n",
    "\n",
    "%time train(model, optimizer, dataloader, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed data parallel\n",
    "Each GPU can be run in it's own process.\n",
    "\n",
    "1. Models are not transferred. \n",
    "1. Each process has it's own model initialized. \n",
    "1. Only gradients are summed up using all_reduce operation\n",
    "1. The seed should be same everywhere for consistency\n",
    "    \n",
    "This should be faster than Dataparallel. The only comppinication cost is all reduce.\n",
    "\n",
    "Tutorial: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n",
    "\n",
    "Data sampler source : https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile dist_training.py\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import os,torch,sys\n",
    "import datetime\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12350'\n",
    "    print('setting up')\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size, timeout=datetime.timedelta(0,seconds =  5))\n",
    "\n",
    "    # Explicitly setting seed to make sure that models created in two processes\n",
    "    # start from same random weights and biases.\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "def run_training(rank, world_size):\n",
    "    print('inside run_training rank {} and world_size {}'.format(rank,world_size))\n",
    "    \n",
    "    #initialize process group and set seed\n",
    "    setup(rank,world_size)\n",
    "    \n",
    "    #print('After setup rank = ', dist.get_rank())\n",
    "    device_id = rank\n",
    "    torch.cuda.set_device(device_id)\n",
    "    \n",
    "    #Initialize model in it's own device. Can be any device. \n",
    "    #Better to initialize in own device for avoiding memory constraints (theory)\n",
    "    model = LinearModel(784,10,verbose = False).to(device_id)\n",
    "    \n",
    "    #Use Distributed data parallel. Check the documentation of DistributedDataParallel?\n",
    "    ddp_model = DistributedDataParallel(model,device_ids=[device_id], output_device=device_id)\n",
    "    \n",
    "    #Check where all_reduce happens\n",
    "    optimizer = torch.optim.Adam(ddp_model.parameters(), lr = args.lr)\n",
    "    \n",
    "    #A sampler is necessary as each process should work only on a subset of data. \n",
    "    #No need to pass rank if process group is initialized. Uses rank = dist.get_rank() to get rank\n",
    "    distributed_sampler = DistributedSampler(train_ds, num_replicas = world_size)\n",
    "    \n",
    "    dataloader = DataLoader(train_ds,batch_size=args.batch, sampler= distributed_sampler)\n",
    "    train(ddp_model, optimizer, dataloader, device_id)\n",
    "    \n",
    "    cleanup()\n",
    "    \n",
    "\n",
    "\n",
    "import argparse\n",
    "if __name__ == '__main2__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--rank', help = 'Rank of the process')\n",
    "    parser.add_argument('--size',help = 'world size')\n",
    "    args = parser.parse_args()\n",
    "    print('starting distibuted training', args)\n",
    "\n",
    "    run_this(args.rank, args.size)\n",
    "    \n",
    "#run_training(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Create 4 processes and start training.\n",
    "If run_training throws cuda error, restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside run_training rank 0 and world_size 1\n",
      "setting up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-12-8ad920580000>\", line 33, in run_training\n",
      "    torch.cuda.set_device(device_id)\n",
      "  File \"/anaconda/envs/py36/lib/python3.6/site-packages/torch/cuda/__init__.py\", line 265, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "RuntimeError: cuda runtime error (3) : initialization error at /opt/conda/conda-bld/pytorch_1556653099582/work/torch/csrc/cuda/Module.cpp:33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Process(Process-4, started)>]\n",
      "Processes finished execution\n",
      "CPU times: user 3.48 ms, sys: 7.82 ms, total: 11.3 ms\n",
      "Wall time: 22.2 ms\n"
     ]
    }
   ],
   "source": [
    "from torch.multiprocessing import Process\n",
    "def test (rank, size):\n",
    "    print('Inside. rank = {}, size = {}'.format(rank,size))\n",
    "def startprocesses(function, ranks, size):\n",
    "    processes = []\n",
    "    for rank in ranks:\n",
    "        p = Process(target=function, args=(rank, size))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    print(processes)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    print('Processes finished execution')\n",
    "    \n",
    "#startprocesses(test, [0,1],2)\n",
    "\n",
    "#One process\n",
    "%time startprocesses(run_training, [0],1)\n",
    "\n",
    "#Two processes\n",
    "#%time startprocesses(run_training, [0,1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_this(0,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
