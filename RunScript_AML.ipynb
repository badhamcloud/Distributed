{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook DistributedDataParallel_script.ipynb to script\n",
      "[NbConvertApp] Writing 11720 bytes to DistributedDataParallel_script.py\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script DistributedDataParallel_script.ipynb\n",
    "#!python DistributedDataParallel_script.py --issingle\n",
    "!rm -rf MNIST_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Azure ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Run\n",
    "ws = Workspace(subscription_id='<subscription id>',\n",
    "              resource_group='bert-base',\n",
    "              workspace_name='SubstrateIntelligenceNLR-WS2')\n",
    "\n",
    "experiment = Experiment(ws, 'MNIST-singlegpu')\n",
    "\n",
    "from azureml.core import ComputeTarget\n",
    "compute_target = ComputeTarget(ws, 'nc24sv3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and upload data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemLog: does data exist True\n",
      "SystemLog: Listdir =  ['MNIST']\n",
      "SystemLog: loading data to ../MNIST_data\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ../MNIST_data\n",
      "    Split: Train\n",
      "60000 10000\n",
      "(784,) 5\n",
      "Uploading an estimated of 6 files\n",
      "Target already exists. Skipping upload for MNIST_data/MNIST\\processed\\test.pt\n",
      "Target already exists. Skipping upload for MNIST_data/MNIST\\processed\\training.pt\n",
      "Target already exists. Skipping upload for MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte\n",
      "Target already exists. Skipping upload for MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte\n",
      "Target already exists. Skipping upload for MNIST_data/MNIST\\raw\\train-images-idx3-ubyte\n",
      "Target already exists. Skipping upload for MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte\n",
      "Uploaded 0 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_aa7c7113d767432b9c2a93827f14f30e"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  DistributedDataParallel_script import loaddata\n",
    "\n",
    "datadir = '../MNIST_data'\n",
    "blobpath = 'MNIST_data/'\n",
    "loaddata(datadir)\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "\n",
    "ds.upload(datadir, target_path=blobpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "framework_version is not specified, defaulting to version 1.1.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "estimator = PyTorch(source_directory = '.',\n",
    "                    pip_packages = ['tqdm'],\n",
    "                    compute_target = compute_target,\n",
    "                    entry_script = 'DistributedDataParallel_script.py',\n",
    "                    script_params = {\n",
    "                       '--issingle':\"\",\n",
    "                       '--datapath':ds.path(blobpath).as_mount()\n",
    "                    },\n",
    "                    use_gpu=True,\n",
    "                   )\n",
    "\n",
    "run = experiment.submit(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ce90aa254c455084b7fea21778fb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed data parallel AML job - Not working\n",
    "2 nodes 4 GPUS\n",
    "Follow this\n",
    "https://github.com/microsoft/AzureML-BERT/blob/master/pretrain/PyTorch/azureml_adapter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "framework_version is not specified, defaulting to version 1.1.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "mpi = MpiConfiguration()\n",
    "mpi.process_count_per_node = 4\n",
    "\n",
    "estimator_distrib = PyTorch(source_directory = '.',\n",
    "                    pip_packages = ['tqdm'],\n",
    "                   compute_target = compute_target,\n",
    "                   entry_script = 'DistributedDataParallel_script.py',\n",
    "                   script_params = {'--isaml':\"\", '--backend' : 'nccl', '--datapath':ds.path(blobpath).as_mount()},\n",
    "                   use_gpu=True,\n",
    "                   distributed_training = mpi,\n",
    "                    node_count = 2\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"script\": \"DistributedDataParallel_script.py\",\n",
       "    \"arguments\": [\n",
       "        \"--isaml\",\n",
       "        \"--backend\",\n",
       "        \"nccl\",\n",
       "        \"--datapath\",\n",
       "        \"$AZUREML_DATAREFERENCE_1d40a0040ea348a084eeba7e9a598b55\"\n",
       "    ],\n",
       "    \"target\": \"nc24sv3\",\n",
       "    \"framework\": \"Python\",\n",
       "    \"communicator\": \"IntelMpi\",\n",
       "    \"maxRunDurationSeconds\": null,\n",
       "    \"nodeCount\": 2,\n",
       "    \"environment\": {\n",
       "        \"name\": null,\n",
       "        \"version\": null,\n",
       "        \"environmentVariables\": {\n",
       "            \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\",\n",
       "            \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
       "            \"NCCL_IB_DISABLE\": \"1\",\n",
       "            \"NCCL_TREE_THRESHOLD\": \"0\"\n",
       "        },\n",
       "        \"python\": {\n",
       "            \"userManagedDependencies\": false,\n",
       "            \"interpreterPath\": \"python\",\n",
       "            \"condaDependenciesFile\": null,\n",
       "            \"baseCondaEnvironment\": null,\n",
       "            \"condaDependencies\": {\n",
       "                \"name\": \"project_environment\",\n",
       "                \"dependencies\": [\n",
       "                    \"python=3.6.2\",\n",
       "                    {\n",
       "                        \"pip\": [\n",
       "                            \"tqdm\",\n",
       "                            \"azureml-defaults\",\n",
       "                            \"torch==1.1\",\n",
       "                            \"torchvision==0.2.1\",\n",
       "                            \"horovod==0.16.1\"\n",
       "                        ]\n",
       "                    }\n",
       "                ],\n",
       "                \"channels\": [\n",
       "                    \"conda-forge\"\n",
       "                ]\n",
       "            }\n",
       "        },\n",
       "        \"docker\": {\n",
       "            \"enabled\": true,\n",
       "            \"baseImage\": \"mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.0-cudnn7-ubuntu16.04\",\n",
       "            \"sharedVolumes\": true,\n",
       "            \"gpuSupport\": true,\n",
       "            \"shmSize\": \"1g\",\n",
       "            \"arguments\": [],\n",
       "            \"baseImageRegistry\": {\n",
       "                \"address\": null,\n",
       "                \"username\": null,\n",
       "                \"password\": null\n",
       "            }\n",
       "        },\n",
       "        \"spark\": {\n",
       "            \"repositories\": [],\n",
       "            \"packages\": [],\n",
       "            \"precachePackages\": false\n",
       "        },\n",
       "        \"databricks\": {\n",
       "            \"mavenLibraries\": [],\n",
       "            \"pypiLibraries\": [],\n",
       "            \"rcranLibraries\": [],\n",
       "            \"jarLibraries\": [],\n",
       "            \"eggLibraries\": []\n",
       "        },\n",
       "        \"inferencingStackVersion\": null\n",
       "    },\n",
       "    \"history\": {\n",
       "        \"outputCollection\": true,\n",
       "        \"snapshotProject\": true,\n",
       "        \"directoriesToWatch\": [\n",
       "            \"logs\"\n",
       "        ]\n",
       "    },\n",
       "    \"spark\": {\n",
       "        \"configuration\": {\n",
       "            \"spark.app.name\": \"Azure ML Experiment\",\n",
       "            \"spark.yarn.maxAppAttempts\": 1\n",
       "        }\n",
       "    },\n",
       "    \"hdi\": {\n",
       "        \"yarnDeployMode\": \"cluster\"\n",
       "    },\n",
       "    \"tensorflow\": {\n",
       "        \"workerCount\": 1,\n",
       "        \"parameterServerCount\": 1\n",
       "    },\n",
       "    \"mpi\": {\n",
       "        \"processCountPerNode\": 4\n",
       "    },\n",
       "    \"dataReferences\": {\n",
       "        \"1d40a0040ea348a084eeba7e9a598b55\": {\n",
       "            \"dataStoreName\": \"workspaceblobstore\",\n",
       "            \"pathOnDataStore\": \"MNIST_data/\",\n",
       "            \"mode\": \"mount\",\n",
       "            \"overwrite\": false,\n",
       "            \"pathOnCompute\": null\n",
       "        }\n",
       "    },\n",
       "    \"sourceDirectoryDataStore\": null,\n",
       "    \"amlcompute\": {\n",
       "        \"vmSize\": null,\n",
       "        \"vmPriority\": null,\n",
       "        \"retainCluster\": false,\n",
       "        \"name\": null,\n",
       "        \"clusterMaxNodeCount\": 2\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_distrib.run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db53f8b942a40edb2c7fe6c098fe2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_distrib = experiment.submit(estimator_distrib)\n",
    "RunDetails(run_distrib).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
